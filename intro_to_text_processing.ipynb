{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4d5faed",
   "metadata": {},
   "source": [
    "# Intro to Text Processing\n",
    "Text processing (or text cleaning) is the crucial first step in any text analysis project. It involves preparing raw text data for analysis by making it more uniform. We'll use string manipulation skills from the last notebook to build a simple cleaning pipeline\n",
    "\n",
    "### The goal of Text Processing\n",
    "Raw text from websites, books or user reviews is often messy. It can contain:\n",
    "- Inconsistent capitalisation (e.g. Apple, apple)\n",
    "- Punctuation Marks that we don't need\n",
    "- Common words (the, a, is) called **stop words** that add little meaning\n",
    "\n",
    "Our goal is to normalise the text to make analysis easier and more accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3276222",
   "metadata": {},
   "source": [
    "## A simple cleaning workflow\n",
    "Let's create a basic function to clean a single piece of text. Our steps will be:\n",
    "1. Convert to lowercase\n",
    "2. Remove punctuation\n",
    "3. Split the text into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa2d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string # Python's string module contains useful constants\n",
    "\n",
    "# Example text\n",
    "document = \"WOW! This is an example sentence. It's for demonstrating text processing, of course.\"\n",
    "\n",
    "# 1. Convert to lowercase\n",
    "doc_lower = document.lower()\n",
    "print(f\"Lowercase:\\n{doc_lower}\")\n",
    "\n",
    "# 2. Remove punctuation\n",
    "# We'll use replace() in a loop\n",
    "for punc in string.punctuation:\n",
    "    doc_lower = doc_lower.replace(punc, \"\")\n",
    "print(f\"\\nNo Punctuation:\\n{doc_lower}\")\n",
    "\n",
    "# 3. Tokenize (split into words)\n",
    "tokens = doc_lower.split()\n",
    "print(f\"\\nTokens:\\n{tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d6750f",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "Stop words are common words that are often filtered out. While there are advanced libraries like NLTK or spaCy for this, we can do it manually with a small, predefined list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac9f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small list of common English stop words\n",
    "stop_words = [\"i\", \"me\", \"my\", \"a\", \"an\", \"the\", \"is\", \"it\", \"in\", \"on\", \"of\", \"for\", \"and\", \"this\", \"that\"]\n",
    "\n",
    "# 'tokens' is the list of words we created in the previous cell\n",
    "filtered_tokens = []\n",
    "for word in tokens:\n",
    "    if word not in stop_words:\n",
    "        filtered_tokens.append(word)\n",
    "\n",
    "print(f\"Tokens after removing stop words:\\n{filtered_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
