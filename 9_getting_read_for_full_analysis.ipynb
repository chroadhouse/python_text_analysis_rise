{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3954f261",
   "metadata": {},
   "source": [
    "# Getting Ready for Analysis - Cleaning and PreProcessing Text\n",
    "In the last notebook we looked at basic features from the raw text and of the news articles. We got a good high-level overview, but to dig deeper, we need to clean up the text\n",
    "\n",
    "## Why do we need to clean text?\n",
    "Computers need consistency. To a computer, \"Election\", \"election\", and \"election.\" are three different words. Our goal is to standardise the text so that we can group and analyze words by their actual meaning. This process is called pre-processing.\n",
    "\n",
    "In this notebook we will build a cleaning pipeline to:\n",
    "- Convert text to lowercase\n",
    "- Remove punctuation and special characters\n",
    "- Break text into a list of individual words (tokenisation)\n",
    "- Remove common, low-value \"stopwords\"\n",
    "- Reduce words to their root form (lemmitisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eeb353",
   "metadata": {},
   "source": [
    "## Setup: Loading the BBC Dataset\n",
    "We'll start by importing our data and load our bbc_df DataFrame, making this notebook self-contained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "212c6bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBC News dataset loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the stable URL\n",
    "url = 'https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv'\n",
    "bbc_df = pd.read_csv(url)\n",
    "\n",
    "print(\"BBC News dataset loaded successfully!\")\n",
    "bbc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f2d214",
   "metadata": {},
   "source": [
    "## The cleaning pipeline\n",
    "We will create new columns in our DataFrame at each step to see how the text transforms. \n",
    "\n",
    "### Step 1: Lowercasing\n",
    "This is the simplest and most crucial first step. It ensures that we don't treat thhe same word differently based on its capitalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b589d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss  left books alone  former worldc...   \n",
       "2  tigers wary of farrell  gamble  leicester say ...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  tv future in the hands of viewers with home th...  \n",
       "1  worldcom boss  left books alone  former worldc...  \n",
       "2  tigers wary of farrell  gamble  leicester say ...  \n",
       "3  yeading face newcastle in fa cup premiership s...  \n",
       "4  ocean s twelve raids box office ocean s twelve...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new column 'cleaned_text' with the lowercase version of the text\n",
    "bbc_df['cleaned_text'] = bbc_df['text'].str.lower()\n",
    "\n",
    "bbc_df[['text', 'cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859fdafe",
   "metadata": {},
   "source": [
    "### Step 2: Removing Punctuation & Special Characters\n",
    "Next, we'll remove all punctuation. This prevents \"win\" and \"win.\" from being treated as two different words. We'll use a simple regular expression to keep only letters, numbers and spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b973e654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss  left books alone  former worldc...   \n",
       "2  tigers wary of farrell  gamble  leicester say ...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  tv future in the hands of viewers with home th...  \n",
       "1  worldcom boss  left books alone  former worldc...  \n",
       "2  tigers wary of farrell  gamble  leicester say ...  \n",
       "3  yeading face newcastle in fa cup premiership s...  \n",
       "4  ocean s twelve raids box office ocean s twelve...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use .str.replace() with a regular expression\n",
    "# The regex [^\\w\\s] means \"any character that is NOT a word character or a whitespace character\"\n",
    "bbc_df['cleaned_text'] = bbc_df['cleaned_text'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "bbc_df[['text', 'cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd385a",
   "metadata": {},
   "source": [
    "### Step 3: Tokenisation\n",
    "Now we'll break our clean string of text into a list of individual words or __tokens__. This is the standard format for most natural language processing tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88258680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>[tv, future, in, the, hands, of, viewers, with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>[worldcom, boss, left, books, alone, former, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>[tigers, wary, of, farrell, gamble, leicester,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>[yeading, face, newcastle, in, fa, cup, premie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>[ocean, s, twelve, raids, box, office, ocean, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss  left books alone  former worldc...   \n",
       "2  tigers wary of farrell  gamble  leicester say ...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [tv, future, in, the, hands, of, viewers, with...  \n",
       "1  [worldcom, boss, left, books, alone, former, w...  \n",
       "2  [tigers, wary, of, farrell, gamble, leicester,...  \n",
       "3  [yeading, face, newcastle, in, fa, cup, premie...  \n",
       "4  [ocean, s, twelve, raids, box, office, ocean, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use .str.split() which splits the string by spaces into a list of words\n",
    "bbc_df['tokenized_text'] = bbc_df['cleaned_text'].str.split()\n",
    "\n",
    "bbc_df[['cleaned_text', 'tokenized_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dd7a9a",
   "metadata": {},
   "source": [
    "### Step 4: Removing Stop Words\n",
    "Stop words are very common words like \"the\", \"a\", \"is\", \"in\", which often don't carry much meaning. Removing them helps us focus on the important keywords. We'll use a standard library for NLP called NLTK (Natural Language Toolkit) to get a list of English stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcd45c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Work\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tokens_no_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[tv, future, in, the, hands, of, viewers, with...</td>\n",
       "      <td>[tv, future, hands, viewers, home, theatre, sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[worldcom, boss, left, books, alone, former, w...</td>\n",
       "      <td>[worldcom, boss, left, books, alone, former, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[tigers, wary, of, farrell, gamble, leicester,...</td>\n",
       "      <td>[tigers, wary, farrell, gamble, leicester, say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[yeading, face, newcastle, in, fa, cup, premie...</td>\n",
       "      <td>[yeading, face, newcastle, fa, cup, premiershi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ocean, s, twelve, raids, box, office, ocean, ...</td>\n",
       "      <td>[ocean, twelve, raids, box, office, ocean, twe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      tokenized_text  \\\n",
       "0  [tv, future, in, the, hands, of, viewers, with...   \n",
       "1  [worldcom, boss, left, books, alone, former, w...   \n",
       "2  [tigers, wary, of, farrell, gamble, leicester,...   \n",
       "3  [yeading, face, newcastle, in, fa, cup, premie...   \n",
       "4  [ocean, s, twelve, raids, box, office, ocean, ...   \n",
       "\n",
       "                                      tokens_no_stop  \n",
       "0  [tv, future, hands, viewers, home, theatre, sy...  \n",
       "1  [worldcom, boss, left, books, alone, former, w...  \n",
       "2  [tigers, wary, farrell, gamble, leicester, say...  \n",
       "3  [yeading, face, newcastle, fa, cup, premiershi...  \n",
       "4  [ocean, twelve, raids, box, office, ocean, twe...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You might need to download the stopwords list the first time you run this\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the standard list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Create a function to remove stop words from a list of tokens\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Apply this function to our tokenized text\n",
    "bbc_df['tokens_no_stop'] = bbc_df['tokenized_text'].apply(remove_stopwords)\n",
    "\n",
    "bbc_df[['tokenized_text', 'tokens_no_stop']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e6afe",
   "metadata": {},
   "source": [
    "### Step 5: Lemmitisation\n",
    "The final steps reduces words to their base or dictionary form (e.g. \"elections\", \"elected\" -> \"election\"). This is powerful way to group words with the same root meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c8ddc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Work\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_no_stop</th>\n",
       "      <th>final_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[tv, future, hands, viewers, home, theatre, sy...</td>\n",
       "      <td>[tv, future, hand, viewer, home, theatre, syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[worldcom, boss, left, books, alone, former, w...</td>\n",
       "      <td>[worldcom, bos, left, book, alone, former, wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[tigers, wary, farrell, gamble, leicester, say...</td>\n",
       "      <td>[tiger, wary, farrell, gamble, leicester, say,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[yeading, face, newcastle, fa, cup, premiershi...</td>\n",
       "      <td>[yeading, face, newcastle, fa, cup, premiershi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ocean, twelve, raids, box, office, ocean, twe...</td>\n",
       "      <td>[ocean, twelve, raid, box, office, ocean, twel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      tokens_no_stop  \\\n",
       "0  [tv, future, hands, viewers, home, theatre, sy...   \n",
       "1  [worldcom, boss, left, books, alone, former, w...   \n",
       "2  [tigers, wary, farrell, gamble, leicester, say...   \n",
       "3  [yeading, face, newcastle, fa, cup, premiershi...   \n",
       "4  [ocean, twelve, raids, box, office, ocean, twe...   \n",
       "\n",
       "                                        final_tokens  \n",
       "0  [tv, future, hand, viewer, home, theatre, syst...  \n",
       "1  [worldcom, bos, left, book, alone, former, wor...  \n",
       "2  [tiger, wary, farrell, gamble, leicester, say,...  \n",
       "3  [yeading, face, newcastle, fa, cup, premiershi...  \n",
       "4  [ocean, twelve, raid, box, office, ocean, twel...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You might need to download wordnet the first time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create a function to lemmatize a list of tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Apply this function to our tokens\n",
    "bbc_df['final_tokens'] = bbc_df['tokens_no_stop'].apply(lemmatize_tokens)\n",
    "\n",
    "bbc_df[['tokens_no_stop', 'final_tokens']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b7b8a6",
   "metadata": {},
   "source": [
    "### Before vs After \n",
    "So why did we do all that. Let's look athe 20 most common words in our dataset _before_ and _after_ cleaning. This will show the power of pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7fddec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top 20 Words (Before Cleaning) ---\n",
      "text\n",
      "the     52567\n",
      "to      24955\n",
      "of      19947\n",
      "and     18561\n",
      "a       18251\n",
      "in      17570\n",
      "s        9007\n",
      "for      8884\n",
      "is       8515\n",
      "that     8135\n",
      "it       7584\n",
      "on       7460\n",
      "was      6016\n",
      "he       5933\n",
      "be       5765\n",
      "with     5313\n",
      "said     5072\n",
      "as       4976\n",
      "has      4952\n",
      "have     4745\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Top 20 Words (After Cleaning) ---\n",
      "final_tokens\n",
      "said          7254\n",
      "mr            3045\n",
      "year          2830\n",
      "would         2577\n",
      "also          2156\n",
      "people        2044\n",
      "new           1970\n",
      "u             1926\n",
      "one           1809\n",
      "could         1511\n",
      "game          1471\n",
      "time          1449\n",
      "last          1381\n",
      "first         1283\n",
      "say           1268\n",
      "world         1214\n",
      "government    1189\n",
      "two           1181\n",
      "company       1113\n",
      "film          1113\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get all words from the original raw text column\n",
    "all_raw_words = bbc_df['text'].str.lower().str.split().explode()\n",
    "\n",
    "# Get all words from our final cleaned tokens column\n",
    "all_cleaned_words = bbc_df['final_tokens'].explode()\n",
    "\n",
    "# Calculate the frequency of the top 20 words for each\n",
    "top_20_raw = all_raw_words.value_counts().head(20)\n",
    "top_20_cleaned = all_cleaned_words.value_counts().head(20)\n",
    "\n",
    "print(\"--- Top 20 Words (Before Cleaning) ---\")\n",
    "print(top_20_raw)\n",
    "print(\"\\n--- Top 20 Words (After Cleaning) ---\")\n",
    "print(top_20_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9acc78",
   "metadata": {},
   "source": [
    "Notice how the _before_ list is dominated by stopwords, while _after_ list contains meaningful keywords like \"said\", \"mr\", \"year\", and \"uk\". Our data is now much more useful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0b223",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "Let's put it all together. \n",
    "\n",
    "Cretae a single function named clean_text that takes a raw text string as input and performs the entire cleaning pipeline:\n",
    "- lowercasing\n",
    "- punctuation removed\n",
    "- tokenisation\n",
    "- stop words removed\n",
    "- lemmatisation\n",
    "\n",
    "Apply this function to the original 'text' column to create a new column and show the head of your Dataframe. \n",
    "\n",
    "We will be using the fuction you create here in future notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20bb115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the Exercise here.\n",
    "# You will need to re-use the stop_words and lemmatizer variables we created earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ceab57",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "You should now have a dataframe that contains a column ('final_tokens') of clean, standardised tokens. This clean data is the standard starting point for almost any advanced NLP task. You are now fully prepared to move onto things such as sentiment analysis, topic modelling and even machine learning if you wanted to. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
