{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd350721",
   "metadata": {},
   "source": [
    "# Going beyond a single word\n",
    "Objective: To understand how analysing sequences of words can provide more context and meaning that analysing single words alone. We will learn how to extract and analyse these sequences, called N-grams\n",
    "\n",
    "### What are N-grams?\n",
    "So far, our analysis (frequency counts, words clouds) have been based on single words or unigrams\n",
    "- __Unigram__: 'prime'\n",
    "- __Bigram__: 'prime minister'\n",
    "- __Trigram__: 'the prime minister'\n",
    "\n",
    "Analysing bigrams and trigrams is powerful because they capture phrases and concepts that single words miss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5908b870",
   "metadata": {},
   "source": [
    "### Setup: Loading and Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608ab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# --- Setup all the cleaning tools ---\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(raw_text):\n",
    "    text = raw_text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(tokens) # Join back to string for vectorizer\n",
    "\n",
    "# --- Load and Clean the Data ---\n",
    "url = 'https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv'\n",
    "bbc_df = pd.read_csv(url)\n",
    "bbc_df['cleaned_text'] = bbc_df['text'].apply(clean_text)\n",
    "\n",
    "print(\"Setup complete. The BBC dataset is loaded and cleaned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc466dcf",
   "metadata": {},
   "source": [
    "## Extracting N-grams with Scikit-learn\n",
    "The easiest way to get N-gram counts is to use scikit-learn's vectoriser. We'll use CountVectorizer for simplicity. The key is the ngram_range parameter\n",
    "- ngram_range=(1,1): Unigrams (default)\n",
    "- ngram_range=(2,2): Bigrams only\n",
    "- ngram_range=(1,2): Both unigrams and bigrams\n",
    "\n",
    "Let's find the most common bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df7312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a CountVectorizer to find bigrams\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2), max_features=20)\n",
    "\n",
    "# Create the bigram document-term matrix\n",
    "bigram_matrix = bigram_vectorizer.fit_transform(bbc_df['cleaned_text'])\n",
    "\n",
    "# Get the feature names (the bigrams)\n",
    "bigram_features = bigram_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Top 20 most frequent bigrams:\")\n",
    "print(bigram_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6305743c",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Find the top 15 most frequent trigrams (sequences of 3 words in the dataset).\n",
    "1. Initialise a new CountVectoriser\n",
    "2. Set the ngram_range parameter to (3,3) to look for trigrams\n",
    "3. Set max_features=15 to get just the top 15\n",
    "4. Fit the vectorizer on the 'cleaned_text' column\n",
    "5. Get the feature names and print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd74afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
