{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e10fb9",
   "metadata": {},
   "source": [
    "# Solution File\n",
    "You can use this file as an easy way to find solutions to each exercise. They will be linked by the title. \n",
    "\n",
    "For some of the tasks, more code than the original exericse will be included in the solution, this will be becuase of code that was already in that sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446fa7af",
   "metadata": {},
   "source": [
    "## Variables and Dataypes \n",
    "\n",
    "#### Exercise One\n",
    "Create and print variables for your own student profile. Include:\n",
    "- Name\n",
    "- Age\n",
    "- Average Grade \n",
    "- Undergraduate or Postgraduate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7255ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: String\n",
    "name = 'James'\n",
    "# Age: Int\n",
    "age = 15\n",
    "# Average Grade\n",
    "avg_grade = 0.8\n",
    "# Undergraduate or Postgraduate: Boolean\n",
    "undergraduate = False\n",
    "\n",
    "print(f'Hi my name is {name} and I am {age} years old')\n",
    "print('I am currently an {}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86a7e8a",
   "metadata": {},
   "source": [
    "## Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4461c9",
   "metadata": {},
   "source": [
    "### Exercise 1: Film Club\n",
    "1. Create a list named **favourite_film** containing three of your favourtie film as strings. \n",
    "2. Add a fourth film to the end of the list using .append()\n",
    "3. Print the very first film in your list \n",
    "4. Print the final list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a list named favourite_films containing three films.\n",
    "#    I've used some common examples here.\n",
    "favourite_films = ['Inception', 'The Matrix', 'Parasite']\n",
    "\n",
    "# 2. Add a fourth film to the end of the list using .append().\n",
    "favourite_films.append('Spider-Man: Into the Spider-Verse')\n",
    "\n",
    "# 3. Print the very first film in your list.\n",
    "#    Remember: the first item is at index 0!\n",
    "print(\"The first film is:\", favourite_films[0])\n",
    "\n",
    "# 4. Print the final list.\n",
    "print(\"The final list of films is:\", favourite_films)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c102e281",
   "metadata": {},
   "source": [
    "### Exercise 2: Film Details\n",
    "1. create a dictionary named film_details for one of the films in your list.\n",
    "2. The dictionary should have keys for 'title', 'director', and 'release_year'.\n",
    "3. Add a new key-value pair to the dictionary: 'language': 'English'.\n",
    "4. Print out the director's name from the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019fb555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. & 2. Create a dictionary with title, director, and release_year.\n",
    "#          I'll base it on the first film from the list above.\n",
    "film_details = {\n",
    "    'title': 'Inception',\n",
    "    'director': 'Christopher Nolan',\n",
    "    'release_year': 2010\n",
    "}\n",
    "\n",
    "# 3. Add a new key-value pair for language.\n",
    "film_details['language'] = 'English'\n",
    "\n",
    "# 4. Print out the director's name from the dictionary.\n",
    "#    We access the value by using its key in square brackets.\n",
    "print(\"The director is:\", film_details['director'])\n",
    "\n",
    "# It's also good practice to print the whole dictionary to see the changes.\n",
    "print(\"The complete film details:\", film_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45704e2",
   "metadata": {},
   "source": [
    "## Control Flow\n",
    "\n",
    "### Exericse 1: Finding Even Numbers\n",
    "1. Create a list of numbers from 1 to 15\n",
    "2. Create an empty list called even_numbers\n",
    "3. Write a for loop that goes through your list of numbers \n",
    "4. Inside the loop, use an if statement to check if a number is even (Hint: A number is even if _number % 2 == 0_).\n",
    "5. If the number is even, add it to the even_numbers list. \n",
    "6. Finally, print the even_numbers list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f775e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a list of numbers from 1 to 15.\n",
    "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "\n",
    "# 2. Create an empty list called even_numbers.\n",
    "even_numbers = []\n",
    "\n",
    "# 3. Write a for loop that goes through the list of numbers.\n",
    "for number in numbers:\n",
    "    # 4. Inside the loop, use an if statement to check if a number is even.\n",
    "    # The modulo operator (%) gives the remainder of a division.\n",
    "    # If a number divided by 2 has a remainder of 0, it's even.\n",
    "    if number % 2 == 0:\n",
    "        # 5. If the number is even, add it to the even_numbers list.\n",
    "        even_numbers.append(number)\n",
    "\n",
    "# 6. Finally, print the even_numbers list.\n",
    "print(even_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b1d71f",
   "metadata": {},
   "source": [
    "### Exercise 2: Words starting with t\n",
    "1. Use the list of words provided below\n",
    "2. Create an empty list called t_words\n",
    "3. Write a for loop to iterate through the word list. \n",
    "4. Inside the loop, use an if statement too check if a word starts with the letter 't'. (Hint: you can get the first letter of a steing with word[0])\n",
    "5. If it does, add the worod to the t_words list.\n",
    "6. Print the t_words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4919531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Use the list of words provided.\n",
    "words_for_exercise = ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
    "\n",
    "# 2. Create an empty list called t_words.\n",
    "t_words = []\n",
    "\n",
    "# 3. Write a for loop to iterate through the words list.\n",
    "for word in words_for_exercise:\n",
    "    # 4. Inside the loop, check if a word starts with 't' or 'T'.\n",
    "    # We get the first letter with word[0].\n",
    "    # We use the .lower() method to make our check case-insensitive.\n",
    "    # This is a very common and important technique in text analysis!\n",
    "    if word[0].lower() == 't':\n",
    "        # 5. If it does, add the original word to the t_words list.\n",
    "        t_words.append(word)\n",
    "\n",
    "# 6. Print the t_words list.\n",
    "print(t_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53a6d6d",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "### Exercise 1: Area Calculator\n",
    "Write a function called calculate_area that takes two parameters, width and height. The function should multiply them together and return the resultling area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a4b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_area(width, height):\n",
    "\n",
    "    area = width * height\n",
    "    return  area \n",
    "\n",
    "\n",
    "print(f'The area of the sqare is {calculate_area(4,4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d5c9ff",
   "metadata": {},
   "source": [
    "### Exercise 2: Word Counter\n",
    "Write a function called count_words that takes a single parameter, text_string. The function should return the number of words in a string (Hint: a word is a sequence of characters seperated by spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae6a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "\n",
    "print(count_words(\"Hello my name is Charlie and I'm learning python\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a77d2f4",
   "metadata": {},
   "source": [
    "## Pandas \n",
    "For these exercises we first need to make sure that you have the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba94b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# 2. Define the URL of the raw CSV data file\n",
    "url = 'https://raw.githubusercontent.com/fivethirtyeight/data/master/fandango/fandango_score_comparison.csv'\n",
    "\n",
    "# 3. Read the data from the URL into a DataFrame\n",
    "movies = pd.read_csv(url)\n",
    "\n",
    "# 4. Display the first 5 rows to confirm it worked\n",
    "print(\"Pandas imported and data loaded successfully!\")\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb2eeff",
   "metadata": {},
   "source": [
    "#### Exercise 1: Shape and Size\n",
    "How many rows and columns are in the movies dataframe ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c63e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .shape attribute returns a tuple where the first item is the number of rows\n",
    "# and the second item is the number of columns.\n",
    "movies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764ccad5",
   "metadata": {},
   "source": [
    "#### Exercise 2: First look\n",
    "Display the first 8 rows of the Dataframe (Hint .head() can take a number as an argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a37040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, .head() shows 5 rows, but you can specify any number you want.\n",
    "movies.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b755105",
   "metadata": {},
   "source": [
    "#### Exercise 3: Specific Columns\n",
    "Create a new dataframe called ratings that contain only the 'FILM', 'IMDB' and Metacritic columns. Display the first 5 rows of this new Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4874446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To select multiple columns, we pass a list of the column names we want.\n",
    "# This is why we use the double square brackets.\n",
    "ratings = movies[['FILM', 'IMDB', 'Metacritic']]\n",
    "\n",
    "# Now, display the head of the new DataFrame to check our work.\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac92fc",
   "metadata": {},
   "source": [
    "#### Exercise 4: Filtering\n",
    "Create a new DataFrame called mc_great_metrics that contains only the movies with a Metacritic score of 90 or higher. Display this new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12762937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we create the condition: movies['Metacritic'] >= 90\n",
    "# Then, we place that condition inside the square brackets of the original DataFrame.\n",
    "# This selects only the rows where the condition is True.\n",
    "mc_great_movies = movies[movies['Metacritic'] >= 90]\n",
    "\n",
    "# Display the new DataFrame to see the results.\n",
    "# Since there are only a few, we can display the whole thing.\n",
    "mc_great_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de815710",
   "metadata": {},
   "source": [
    "## Basic Text Analysis \n",
    "Before we can solve the exercises, we need to set up our environment. This cell performs the steps from the main notebook to laod the BBC News dataset and create the feature columns that our exercises rely on. Run this single cell first to get everything ready. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4446ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: Load the Data ---\n",
    "# This URL points to a stable dataset hosted on Google Cloud Storage.\n",
    "url = 'https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv'\n",
    "bbc_df = pd.read_csv(url)\n",
    "\n",
    "# --- Step 2: Re-create all the Feature Columns ---\n",
    "\n",
    "# Calculate character count\n",
    "bbc_df['char_count'] = bbc_df['text'].str.len()\n",
    "\n",
    "# Calculate word count\n",
    "bbc_df['word_count'] = bbc_df['text'].str.split().apply(len)\n",
    "\n",
    "# Calculate the punctuation count\n",
    "bbc_df['punc_count'] = bbc_df['text'].str.count(r'[.,!?:;-]')\n",
    "\n",
    "# Define the function to calculate average word length\n",
    "def calculate_avg_word_length(text):\n",
    "    words = text.split()\n",
    "    total_chars = len(\"\".join(words))\n",
    "    total_words = len(words)\n",
    "    if total_words == 0:\n",
    "        return 0\n",
    "    return total_chars / total_words\n",
    "\n",
    "# Apply the function to create the avg_word_length column\n",
    "bbc_df['avg_word_length'] = bbc_df['text'].apply(calculate_avg_word_length)\n",
    "\n",
    "\n",
    "print(\"Setup complete! The 'bbc_df' DataFrame is ready with all feature columns.\")\n",
    "bbc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb3672",
   "metadata": {},
   "source": [
    "#### Exercise 1: The Longest Article\n",
    "Find the word count of the longest article in the entire dataset. (Hint: use the .max() method on the 'word_count' column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aebc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 'word_count' column and call the .max() method\n",
    "longest_article_wc = bbc_df['word_count'].max()\n",
    "\n",
    "print(f\"The word count of the longest article is: {longest_article_wc} words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb30e9d7",
   "metadata": {},
   "source": [
    "#### Exercise 2: Overall Average Word Count\n",
    "What is the average word_count for an article across the whole dataset? (Hint: use the .mean() method on the 'word_count' column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 'word_count' column and call the .mean() method\n",
    "average_word_count = bbc_df['word_count'].mean()\n",
    "\n",
    "# We can use an f-string to format the output to a whole number for clarity\n",
    "print(f\"The average word count for an article is: {average_word_count:.0f} words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dc19a8",
   "metadata": {},
   "source": [
    "#### Exercise 3: Averages by Category (Advanced)\n",
    "This dataset has a 'category' column. Can you calculate the average word count for each category? This is a very common task in data analysis.\n",
    "\n",
    "(Hint: The easiest way to do this is with Pandas' .groupby() method: bbc_df.groupby('category')['word_count'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f31f458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by the 'category' column.\n",
    "# Then, select the 'word_count' column for those groups.\n",
    "# Finally, calculate the mean() for each group.\n",
    "avg_wc_by_category = bbc_df.groupby('category')['word_count'].mean()\n",
    "\n",
    "print(\"Average word count per category:\")\n",
    "print(avg_wc_by_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0d6973",
   "metadata": {},
   "source": [
    "# Data cleaning and pre-processing\n",
    "Before we are able to provide an exercise solution we first must ensure that we have the correct things such as libraries and data imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5472d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# --- Download NLTK data (only needs to be done once) ---\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# --- Prepare NLP objects ---\n",
    "# Get the standard list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# --- Load the Data ---\n",
    "# This URL points to a stable dataset hosted on Google Cloud Storage.\n",
    "url = 'https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv'\n",
    "bbc_df = pd.read_csv(url)\n",
    "\n",
    "print(\"Setup complete! The environment is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a2f558",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "Let's put it all together. \n",
    "\n",
    "Cretae a single function named clean_text that takes a raw text string as input and performs the entire cleaning pipeline:\n",
    "- lowercasing\n",
    "- punctuation removed\n",
    "- tokenisation\n",
    "- stop words removed\n",
    "- lemmatisation\n",
    "\n",
    "Apply this function to the original 'text' column to create a new column and show the head of your Dataframe. \n",
    "\n",
    "We will be using the fuction you create here in future notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf88f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw_text):\n",
    "    \"\"\"\n",
    "    Performs the full cleaning pipeline on a single text string.\n",
    "    1. Lowercases the text.\n",
    "    2. Removes punctuation and special characters.\n",
    "    3. Tokenizes the text.\n",
    "    4. Removes stop words.\n",
    "    5. Lemmatizes the tokens.\n",
    "    Returns a list of clean tokens.\n",
    "    \"\"\"\n",
    "    # Step 1: Lowercasing\n",
    "    text = raw_text.lower()\n",
    "    \n",
    "    # Step 2: Removing Punctuation & Special Characters\n",
    "    # The re.sub function is used for regular expression substitutions\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Step 3: Tokenization\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Step 4: Removing Stop Words\n",
    "    # We use a list comprehension for a concise way to filter the list\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Step 5: Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply the function to the original 'text' column\n",
    "bbc_df['final_tokens_solution'] = bbc_df['text'].apply(clean_text)\n",
    "\n",
    "\n",
    "# Display the original text and the result of our function to verify it works\n",
    "display(bbc_df[['text', 'final_tokens_solution']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aad9d1",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "Before we are able to provide the solution of the code, we need to have some preamble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6473c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download the VADER lexicon (only needs to be done once)\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Load the BBC News dataset from the stable URL\n",
    "url = 'https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv'\n",
    "bbc_df = pd.read_csv(url)\n",
    "\n",
    "# Initialize the VADER sentiment intensity analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Create the 'sentiment_score' column by applying VADER to the 'text' column\n",
    "bbc_df['sentiment_score'] = bbc_df['text'].apply(lambda text: sid.polarity_scores(text)['compound'])\n",
    "\n",
    "print(\"Setup complete! The 'bbc_df' DataFrame is ready with the 'sentiment_score' column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa78cdc",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Let's categorise each article and count them up \n",
    "1. Create the label_sentiment function from the previous notebook that takes a score and returns 'Positive', 'Negative', or 'Neutral' (using the +/- 0.05 threshold)\n",
    "2. Apply this function to the 'sentiment_score' column to create a new 'sentiment_label' column\n",
    "3. Use .value_counts() on the new 'sentiment_label' column. What percentage of articles fall into each category ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5550cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a function that takes a score and returns a label.\n",
    "def label_sentiment(score):\n",
    "    if score > 0.05:\n",
    "        return \"Positive\"\n",
    "    elif score < -0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# 2. Apply this function to the 'sentiment_score' column to create the new label column.\n",
    "bbc_df['sentiment_label'] = bbc_df['sentiment_score'].apply(label_sentiment)\n",
    "\n",
    "\n",
    "# 3. Use .value_counts() to see the number of articles in each category.\n",
    "print(\"--- Count of Articles per Sentiment Category ---\")\n",
    "label_counts = bbc_df['sentiment_label'].value_counts()\n",
    "print(label_counts)\n",
    "\n",
    "\n",
    "# 4. Use .value_counts(normalize=True) to see the percentage of articles.\n",
    "print(\"\\n--- Percentage of Articles per Sentiment Category ---\")\n",
    "label_percentages = bbc_df['sentiment_label'].value_counts(normalize=True) * 100\n",
    "print(label_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a605fb",
   "metadata": {},
   "source": [
    "# Text visualisaiton\n",
    "Here we have the preamble before the exercise \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8231e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Install necessary libraries ---\n",
    "!pip install wordcloud\n",
    "\n",
    "# --- Step 2: Import all libraries ---\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# --- Step 3: Download NLTK data (only needs to be done once) ---\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# --- Step 4: Define the cleaning function ---\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(raw_text):\n",
    "    text = raw_text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# --- Step 5: Load and process the data ---\n",
    "url = 'https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv'\n",
    "bbc_df = pd.read_csv(url)\n",
    "bbc_df['final_tokens'] = bbc_df['text'].apply(clean_text)\n",
    "\n",
    "print(\"Setup complete! The environment is ready and the data is processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e474be97",
   "metadata": {},
   "source": [
    "## Exericse\n",
    "The overall word cloud is interesting, but what if we need to see the key theme for a single category ? \n",
    "Task:\n",
    "1. Create a new DataFrame that only contains the artciles from the 'sport' category\n",
    "2. Generate a single string of text containgin all the cleaned words from just the sport category \n",
    "3. Create and display a word cloud for the sport category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be9822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a new DataFrame that contains only the 'sport' articles.\n",
    "# We use boolean indexing to filter the DataFrame.\n",
    "sport_df = bbc_df[bbc_df['category'] == 'sport']\n",
    "\n",
    "\n",
    "# 2. Generate a single string of text from the cleaned sport articles.\n",
    "# First, we get a list of all words from the 'final_tokens' of our new sport_df.\n",
    "all_sport_words = sport_df['final_tokens'].explode()\n",
    "# Then, we join them all together into one big string, separated by spaces.\n",
    "sport_text_for_wordcloud = \" \".join(all_sport_words)\n",
    "\n",
    "\n",
    "# 3. Create and display the word cloud for the sport category.\n",
    "# Initialize the WordCloud object.\n",
    "sport_wordcloud = WordCloud(width=800, height=400, background_color='white', collocations=False).generate(sport_text_for_wordcloud)\n",
    "# Note: collocations=False helps prevent common pairs of words from dominating the cloud.\n",
    "\n",
    "# Use matplotlib to display the generated image.\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(sport_wordcloud, interpolation='bilinear')\n",
    "plt.title('Word Cloud for the \"Sport\" Category')\n",
    "plt.axis(\"off\") # Hides the x and y axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ada18",
   "metadata": {},
   "source": [
    "# N-grams\n",
    "Here we have the preamble and then we can get to the solution of the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef36958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# --- Setup all the cleaning tools ---\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(raw_text):\n",
    "    text = raw_text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(tokens) # Join back to string for vectorizer\n",
    "\n",
    "# --- Load and Clean the Data ---\n",
    "url = 'https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv'\n",
    "bbc_df = pd.read_csv(url)\n",
    "bbc_df['cleaned_text'] = bbc_df['text'].apply(clean_text)\n",
    "\n",
    "print(\"Setup complete. The BBC dataset is loaded and cleaned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3787154b",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Find the top 15 most frequent trigrams (sequences of 3 words in the dataset).\n",
    "1. Initialise a new CountVectoriser\n",
    "2. Set the ngram_range parameter to (3,3) to look for trigrams\n",
    "3. Set max_features=15 to get just the top 15\n",
    "4. Fit the vectorizer on the 'cleaned_text' column\n",
    "5. Get the feature names and print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a52f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a new DataFrame with only the politics articles\n",
    "politics_df = bbc_df[bbc_df['category'] == 'politics']\n",
    "\n",
    "# 1. Initialize a new CountVectorizer for trigrams.\n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(3, 3), max_features=15)\n",
    "\n",
    "# 2. Fit the vectorizer on the cleaned text of the politics articles.\n",
    "trigram_vectorizer.fit(politics_df['cleaned_text'])\n",
    "\n",
    "# 3. Get the feature names (the top 15 trigrams) and print them.\n",
    "top_trigrams = trigram_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Top 15 most frequent trigrams in the 'politics' category:\")\n",
    "print(top_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d05519f",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "Before we have the solution we need to make sure we have the data and libraries imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e2040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Import all necessary libraries ---\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# --- Step 2: Download NLTK data (only needs to be done once) ---\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# --- Step 3: Define the cleaning function ---\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(raw_text):\n",
    "    text = raw_text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# --- Step 4: Load and process the data ---\n",
    "url = 'https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv'\n",
    "bbc_df = pd.read_csv(url)\n",
    "bbc_df['final_tokens'] = bbc_df['text'].apply(clean_text)\n",
    "\n",
    "# --- Step 5: Vectorize the Text using TF-IDF ---\n",
    "# Join the tokens back into strings for the vectorizer\n",
    "bbc_df['cleaned_text_joined'] = bbc_df['final_tokens'].apply(lambda tokens: \" \".join(tokens))\n",
    "\n",
    "# Initialize and fit the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(bbc_df['cleaned_text_joined'])\n",
    "\n",
    "# --- Step 6: Define the helper function to print topics ---\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = f\"Topic #{topic_idx}: \"\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "print(\"Setup complete! The environment is ready with all necessary data and functions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe4b810",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "The model worked great when we told it find 5 topics, which we know is the 'correct' number for this dataset\n",
    "\n",
    "But in the real-world, you don't know the number of topics before we start, so we usually have to experiment. \n",
    "The task:\n",
    "1. Re-run the LDA model, but this time set n_components=3\n",
    "2. Print the top words for these 3 \n",
    "3. Do the topics seem as clear and distinct as they were? Or have they become furuther jumbled ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcefe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Re-run the LDA model with n_components=3\n",
    "num_topics_exercise = 3\n",
    "lda_exercise = LatentDirichletAllocation(n_components=num_topics_exercise, random_state=42)\n",
    "lda_exercise.fit(tfidf_matrix)\n",
    "\n",
    "\n",
    "# 2. Print the top words for these 3 topics.\n",
    "# Get the feature names (our vocabulary) from the vectorizer created in the preamble\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Top words for {num_topics_exercise} topics:\")\n",
    "print_top_words(lda_exercise, feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e621d",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c70cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the spaCy library and download the English model\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Import necessary libraries\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the BBC dataset\n",
    "url = 'https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv'\n",
    "bbc_df = pd.read_csv(url)\n",
    "\n",
    "print(\"Setup complete. spaCy is ready and the BBC dataset is loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5aed47",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "The task is to find all organisations in the first article from 'tech'\n",
    "1. Select the text of the first article when the category is 'tech'\n",
    "2. Process it with the nlp object create a doc\n",
    "3. Create an empty list called tech_organisation \n",
    "4. Loop thorugh the enntities in the doc (doc.ents). If any entities labels is 'ORG', append its text (ent.text) to your list\n",
    "5. Print the final list of organisation names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bb569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select the text of the first article where the category is 'tech'.\n",
    "tech_article_text = bbc_df[bbc_df['category'] == 'tech']['text'].iloc[0]\n",
    "\n",
    "# 2. Process it with the nlp object to create a doc.\n",
    "doc = nlp(tech_article_text)\n",
    "\n",
    "# 3. Create an empty list to store the results.\n",
    "tech_organizations = []\n",
    "\n",
    "# 4. Loop through the entities and check for the 'ORG' label.\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'ORG':\n",
    "        tech_organizations.append(ent.text)\n",
    "        \n",
    "# 5. Print the final list of organization names.\n",
    "print(\"Organizations found in the first tech article:\")\n",
    "print(tech_organizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d139a3",
   "metadata": {},
   "source": [
    "# Text Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df44df37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def simple_clean(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Load and clean the data\n",
    "url = 'https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv'\n",
    "bbc_df = pd.read_csv(url)\n",
    "bbc_df['cleaned_text'] = bbc_df['text'].apply(simple_clean)\n",
    "\n",
    "# Vectorize the text with TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(bbc_df['cleaned_text'])\n",
    "\n",
    "# Calculate the cosine similarity between all documents\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Define the recommendation function\n",
    "def find_similar_articles(article_index, top_n=5):\n",
    "    similar_scores = list(enumerate(cosine_sim_matrix[article_index]))\n",
    "    sorted_similar_scores = sorted(similar_scores, key=lambda x: x[1], reverse=True)\n",
    "    top_indices = [i[0] for i in sorted_similar_scores[1:top_n+1]]\n",
    "    print(f\"--- Top {top_n} articles similar to Article {article_index} ---\")\n",
    "    print(f\"ORIGINAL ({bbc_df['category'].iloc[article_index]}): {bbc_df['text'].iloc[article_index][:100]}...\")\n",
    "    print(\"-\" * 50)\n",
    "    return bbc_df.iloc[top_indices]\n",
    "\n",
    "print(\"Setup complete. Text has been vectorized and similarity function is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb6e95d",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "You task is to use the find_similar_articles function to find the top 3 most similar articles to Article 500\n",
    "- Call the function with theh correct parameters(artcile_index=500, top_n=3)\n",
    "- Look at the category of the original article and the categories of the recommended articles  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24baf79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's see what the original article is about\n",
    "print(\"--- Original Article #500 ---\")\n",
    "print(f\"Category: {bbc_df['category'].iloc[500]}\")\n",
    "print(f\"Text: {bbc_df['text'].iloc[500][:150]}...\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Now, find the 3 most similar articles\n",
    "similar_articles = find_similar_articles(article_index=500, top_n=3)\n",
    "\n",
    "# Display the results\n",
    "display(similar_articles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
